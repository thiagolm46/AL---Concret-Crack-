{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classificação-Concrete Crack.pynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJY_tbc2PBDB"
      },
      "source": [
        "# Classificação - Concrete Crack\n",
        "Banco de dados utilizado: https://data.mendeley.com/datasets/5y9wdsg2zt/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0sTNIbMHK-3"
      },
      "source": [
        "## Processamento das Imagens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGLmlsxNPFvK"
      },
      "source": [
        "**Bibliotecas Necessárias**\n",
        "- os - Acessar arquivos do computador\n",
        "- numpy - Funções Algébricas e mexer com Matrizes\n",
        "- matploitlib - Gráficos\n",
        "- time - Tempo de execução\n",
        "- PIL - Modificar imagens\n",
        "- sklearn.model_selection - Dividir dados entre teste e treino\n",
        "- numba - Agilizar processamento de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsNl61gpO2MS"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numba import njit, prange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwqJYd_uPhyo"
      },
      "source": [
        "**Processamento das Imagens**\n",
        "1. Redimensiona para resolução de 32x32\n",
        "2. Transforma a imagem cinza\n",
        "3. Transforma em array do tipo float\n",
        "4. Transforma o array em uma linha (coluna abaixo de coluna)\n",
        "5. Normaliza os valores\n",
        "6. Adiciona o array embaixo da matriz dados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSDPnZSgS4-I"
      },
      "source": [
        "def obter_dados(input_dir):\n",
        " \n",
        "    # Obtém lista de arquivos do diretório\n",
        "    lista_de_arquivos = os.listdir(input_dir)\n",
        "    \n",
        "    # Obtém um vetor com lixo de memória para imagens serem adicionadas lado a lado\n",
        "    dados = np.empty(shape=[1, 32*32], dtype=int)\n",
        " \n",
        "    for arquivo in lista_de_arquivos:\n",
        " \n",
        "        # Abre imagem do diretório\n",
        "        imagem = Image.open(os.path.join(input_dir, arquivo))\n",
        " \n",
        "        # Redimensiona imagem\n",
        "        imagem_redimensionada = imagem.resize((32, 32))\n",
        " \n",
        "        # Torna imagem redimensionada cinza\n",
        "        # .convert('L') utiliza somente a luminosidade, converte para tons de cinza\n",
        "        imagem_cinza = imagem_redimensionada.convert('L')\n",
        " \n",
        "        # Transforma imagem em array com valores em float\n",
        "        imagem_array = np.asarray(imagem_cinza, dtype='float')\n",
        "        \n",
        "        # Normaliza os valores para ficar entre valores entre 0 e 1\n",
        "        imagem_array /= 255.0\n",
        " \n",
        "        # Ordena coluna embaixo de coluna e transforma em uma linha\n",
        "        uma_linha = imagem_array.reshape((1, 1024), order='F')\n",
        " \n",
        "        # Adiciona nova linha embaixo da matriz dados\n",
        "        dados = np.vstack((dados, uma_linha))\n",
        "        \n",
        "    # Retorna dados e deleta primeira linha que possui valores lixo de memória\n",
        "    return np.delete(dados,(0), axis=0)\n",
        " \n",
        " \n",
        "diretorio_negative = '/Content/Concrete_crack/Negative'\n",
        "diretorio_positive = '/Content/Concrete_crack/Positive'\n",
        " \n",
        "dados_positive = obter_dados(diretorio_positive)\n",
        "dados_negative = obter_dados(diretorio_negative)\n",
        " \n",
        "# Junta dados com rachadura e sem rachadura\n",
        "dados = np.vstack((dados_positive, dados_negative))\n",
        " \n",
        "# Salva matriz de dados amostras x características\n",
        "np.save(os.path.join('Content/Concrete_crack', 'dados.npy'), dados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w7iRKaKzY10"
      },
      "source": [
        "Com a matriz dados salva em um arquivo numpy, podemos caregá-la rapidamente abrindo tal arquivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PqSko716DGQ"
      },
      "source": [
        "# Carregar matriz dados a partir do arquivo dados.npy\n",
        "dados = np.load('/content/Concrete_crack/dados.npy')\n",
        "\n",
        "# Imprime matrizes obtidas\n",
        "print(f'Matriz dados:\\n\\n{dados}\\n\\nDimensões: {dados.shape}')\n",
        "print('Data Type: %s' % dados.dtype)\n",
        "print('Min: %.3f, Max: %.3f' % (dados.min(), dados.max()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15s10A_Ezv9d"
      },
      "source": [
        "Calculamos a matriz y, que armazena os resultados, com 1 para imagens com rachaduras e 0 para imagens sem rachaduras.\n",
        "\n",
        "Dividimos as amostras em treino e teste (proporção de 60%/40%). \n",
        "\n",
        "A partir de agora, usaremos apenas a matriz X_train (treino) para mostrarmos diversas propriedades das matrizes ortogonais e das projeções ortogonais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvpv9Ge1xdfj"
      },
      "source": [
        "# Matriz y (resultados)\n",
        "# 1 = Positivo para Rachadura\n",
        "y = np.ones(20000).reshape(-1, 1)\n",
        "# 0 = Negativo para Rachadura\n",
        "y = np.vstack((y, np.zeros(20000).reshape(-1,1)))\n",
        "\n",
        "# Dividir dados em teste e treino\n",
        "X_train, X_test, y_train, y_test = train_test_split(dados, y, test_size=0.40, random_state=0)\n",
        "\n",
        "# Imprime dimensões das matrizes obtidas\n",
        "print(f'Dimensões:\\n\\nX treino: {X_train.shape}\\nX teste: {X_test.shape}')\n",
        "print(f'\\ny treino: {y_train.shape}\\ny teste: {y_test.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmP2ayuREE1C"
      },
      "source": [
        "## Tarefas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPLjHA3OEe99"
      },
      "source": [
        "\n",
        "### 1. Centralizar a matriz de dados original $\\tilde{X}$ obtendo a matriz X.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Qvf4dTaFS2"
      },
      "source": [
        "Resolução:\n",
        "\n",
        "Para centralizar a matriz, calculamos o valor médio de cada variável, localizadas nas colunas, com a função numpy.mean, tendo como argumento axis=0, o qual indica que será calculada a média dos valores das colunas e em seguida subtraímos do conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Amor3r_EgGX"
      },
      "source": [
        "# Centraliza-se os dados em relação à média da respectiva coluna\n",
        "\n",
        "X_train_mean = np.mean(X_train, axis=0)\n",
        "X_train = X_train - X_train_mean\n",
        "\n",
        "X_test_mean = np.mean(X_test, axis=0)\n",
        "X_test = X_test - X_test_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwgtOsufEgoe"
      },
      "source": [
        "### 2. Calcular a matriz de covariância dos dados de duas maneiras:\n",
        "- $np.cov(\\tilde{X}$),\n",
        "- $\\frac{1}{m-1}X^TX$\n",
        "\n",
        "Comparar os resultados. Atenção: verifique se o parâmetro rowvar na\n",
        "função np.cov( ) deve ser True ou False.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE4Wxu-dXqCg"
      },
      "source": [
        "Resolução:\n",
        "\n",
        "Para calcularmos a matriz de covariância, comparamos duas maneiras, na primeira utilizamos a função numpy.cov nos dados não centralizados, com o paramêtro rowvar=False, já que as variáveis estão localizadas nas colunas, e na outra forma utilizamos a fórmula $\\frac{1}{1-m}X^TX$ nos dados centralizados. \n",
        "\n",
        "A Matriz resultante da subtração das duas matrizes de covariância, caso essas sejam iguais, deve dar próxima da matriz nula. Para verificarmos, calculamos o valor absoluto máximo de tal matriz, que deu 2.9976021664879227e-15, ou seja, um número muito pŕoximo de zero, e uma diferença desprezível resultante da imprecisão do tipo float.\n",
        "\n",
        "Visto isso, os dois métodos são viáveis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4CGAv_TEl7Q"
      },
      "source": [
        "# Matriz covariância utilizando np.cov\n",
        "matriz_covariancia_1 = np.cov(X_train + X_train_mean, rowvar=False)\n",
        "\n",
        "# m = numero de linhas\n",
        "m = np.size(X_train, 0)\n",
        "\n",
        "fator = 1 / (m - 1)\n",
        "\n",
        "# Matriz covariância utilizando fórmula algébrica\n",
        "matriz_covariancia_2 = fator * np.transpose(X_train) @ X_train\n",
        "\n",
        "print(f'Matriz Covariância utilizando np.cov:\\n\\n{matriz_covariancia_1}\\n')\n",
        "print(f'Matriz Covariância utilizando fórmula algébrica:\\n\\n{matriz_covariancia_2}\\n')\n",
        "\n",
        "# Valor absoluto máximo da subtração das duas matrizes\n",
        "# Para comparação das matrizes\n",
        "maximo_absoluto = np.max(np.abs(matriz_covariancia_1 - matriz_covariancia_2))\n",
        "\n",
        "print(f'\\nNúmero máximo - diferença: {maximo_absoluto}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81dCB5BmEmxf"
      },
      "source": [
        "### 3. Calcular a decomposição espectral da matriz de covariância dos dados, isto é,  $cov(X) = QΛQ^T$, usando a função do NumPy: np.linalg.eigh(cov(X)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA7brjgTml6i"
      },
      "source": [
        "Resolução:\n",
        "\n",
        "Para calcular a decomposição espectral, ou seja, obter os autovalores e autovetores, usamos a função np.linalg.eigh, com a matriz de covariância como parâmetro.\n",
        "\n",
        "Após obtermos os autovalores e autovetores, ordenamos eles de acordo com a  ordem decrescente dos autovalores, que fora obtido calculando-se a ordem inversa do retorno da função np.argsort, a qual ordena a matriz em ordem crescente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpwvV7kAFX6p"
      },
      "source": [
        "# Obtém os autovalores e autovetores\n",
        "w, Q = np.linalg.eigh(np.cov(X_train, rowvar=False))\n",
        "\n",
        "# Obtém os índices para ordenação decrescente dos autovalores\n",
        "indices = np.argsort(w)[::-1]\n",
        "\n",
        "# Ordena autovalores e autovetores em ordem decrescente\n",
        "w = w[indices]\n",
        "Q = Q[indices]\n",
        "\n",
        "print(f'Autovalores:\\n\\n{w}\\n')\n",
        "print(f'Autovetores:\\n\\n{Q}')\n",
        "print(f'Tempo de execução: {time.time()-inicio}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMLupshkF1mW"
      },
      "source": [
        "## Questões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl9x1v66F30O"
      },
      "source": [
        "2. Vericar, com o respectivo banco de dados, que $Q^TQ ≈ I.$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aJlyqCYn_bc"
      },
      "source": [
        "Resolução:\n",
        "\n",
        "Para verificarmos a que Q realmente é ortogonal, ou seja, $Q^TQ ≈ I$, calculamos a matriz diferença entre $Q^TQ$ e $I$, que deve resultar em uma matriz próxima da nula, caso essas matrizes sejam muito pŕoximas. \n",
        "\n",
        "A fim de verificarmos isso, calculamos o número absoluto máximo presente na matriz diferença, 1.0436096431476471e-14, o qual, por ser muito próximo de zero, com uma diferença desprezível resultante da imprecisão do tipo float, indica que as matrizes são muito parecidas.\n",
        "\n",
        "Visto isso, a matriz Q é ortogonal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSaRBK4aGICT"
      },
      "source": [
        "# Calcula Q transposto Q\n",
        "teste_ortogonal = np.transpose(Q) @ Q \n",
        "\n",
        "# Identidade de mesmas dimensões\n",
        "identidade = np.identity(1024, dtype='float')\n",
        "\n",
        "# Matriz teste se tais matrizes são iguais/parecidas\n",
        "maximo_absoluto = np.max(np.abs(teste_ortogonal - identidade))\n",
        "\n",
        "print(f'Número máximo - diferença: {maximo_absoluto}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1hLRGrvGIhJ"
      },
      "source": [
        "### 5. Calcular, com o respectivo banco de dados, as matrizes $\\hat{Z}$ e $\\hat{X}$; vericar que a matriz de projeção $$\\hat{Q}\\hat{Q}^T$$ **não** é a matriz identidade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8bgoFjWmjfz"
      },
      "source": [
        "Resolução:\n",
        "\n",
        "Para calculamos $\\hat{Q}$, deletamos metade das colunas de Q, obtendo uma matriz de dimensões 1024x512. \n",
        "\n",
        "Já em $\\hat{Z}$, utilizamos a fórmula $\\hat{Z} =  X\\hat{Q}$, obtendo uma matriz de dimensões 24000x512\n",
        "\n",
        "Em $\\hat{X}$, utilizamos a fórmula $\\hat{X} = X\\hat{Q}\\hat{Q}^T$, obtendo uma matriz de dimensões 24000x1024\n",
        "\n",
        "Com o objetivo de verificarmos se $\\hat{Q}$ é ortogonal, subtraímos a matriz $\\hat{Q}^T\\hat{Q}$ da matriz identidade $I$, e obtemos como número absoluto maxímo da matriz diferença 0.5660154821287144, o que indica que $\\hat{Q}^T\\hat{Q}$ não é a matriz identidade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00_SbTdjRYZc"
      },
      "source": [
        "# Deleta metade das colunas de Q (colunas de índices pares)\n",
        "Q_chapeu = np.delete(Q,np.s_[::2], 1)\n",
        "\n",
        "# Calcula Z chapeu\n",
        "Z_chapeu = X_train @ Q_chapeu\n",
        "\n",
        "# Calcula X chapeu\n",
        "X_chapeu = X_train @ Q_chapeu @ np.transpose(Q_chapeu)\n",
        "\n",
        "# Calcula Qchapeu * Qchapeu transpoto\n",
        "teste_ortogonal = Q_chapeu @ np.transpose(Q_chapeu) \n",
        "\n",
        "# Identidade de mesmas dimensões\n",
        "identidade = np.identity(1024, dtype='float')\n",
        "\n",
        "# Matriz teste se tais matrizes são iguais/parecidas\n",
        "teste_igualdade = teste_ortogonal - identidade\n",
        "\n",
        "# Resultados\n",
        "print(f'Matriz Q chapéu: \\n\\n{Q_chapeu}\\nDimensões: {Q_chapeu.shape}\\n')\n",
        "print(f'Matriz Z chapéu: \\n\\n{Z_chapeu}\\nDimensões: {Z_chapeu.shape}\\n')\n",
        "print(f'Matriz X chapéu: \\n\\n{X_chapeu}\\nDimensões: {X_chapeu.shape}\\n')\n",
        "print(f'\\nVerificação Q chapéu * Q chapéu tranposto\\n\\nNúmero máximo - diferença: {np.max(np.abs(teste_igualdade))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRlsYqhGzMwo"
      },
      "source": [
        "# Trabalho Final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG4LbM7MzPju"
      },
      "source": [
        "## Tarefas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV6_sTQazSFf"
      },
      "source": [
        "### 01. Calcular, para o respectivo banco de dados, a SVD da matriz de dados centralizados X. Gerar um gráfico de número de valores singulares versus variabilidade acumulada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8kAY0SIwpXL"
      },
      "source": [
        "Para realizar a decomposição em valores singulares(SVD), utilizamos a função np.linalg.svd. Após receber as matrizes U, S e V transposto, ordenamos S em ordem decrescente e, a partir disso, ordenamos a matriz V. Os resultados foram uma matriz U de 24000 por 1024, um vetor S de 1024, o qual representa a diagonal principal da matriz que contém os autovalores na decomposição svd, e a matriz V transposta de 1024 por 1024.\n",
        "\n",
        "Para calcular a variabilidade acumulada, realizamos a divisão entre a soma acumulada de autovalores e a soma de dos mesmos, utilizando a  função np.cumsum de S ao quadrado e a função np.sum de S ao quadrado, e geramos um gráfico utilizando a biblioteca matplotlib, de valores singulares X variabilidade acumulada. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skMVtONzUL3"
      },
      "source": [
        "# Calcula a SVD da matriz\n",
        "U, S, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
        "\n",
        "# Obtém os índices para ordenação decrescente dos valores singulares de S\n",
        "indices = np.argsort(S)[::-1]\n",
        "\n",
        "# Ordena S e V em ordem decrescente\n",
        "S = S[indices]\n",
        "Vt = Vt[indices,:]\n",
        "\n",
        "print('Dimensões: \\nU: {} \\nS: {} \\nVt: {}' \\\n",
        "      .format(U.shape, S.shape, Vt.shape) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFZTVlR-sECa"
      },
      "source": [
        "# Array com: Soma acumulada / Soma de valores singulares\n",
        "variabilidade_acumulada = np.cumsum(S**2) / np.sum(S**2)\n",
        "\n",
        "# Número de autovalores\n",
        "x_plot = range(len(S))\n",
        "\n",
        "# Criar Gráfico\n",
        "plt.plot(x_plot, 100 * variabilidade_acumulada)\n",
        "plt.xlabel('Numero de Valores Singulares')\n",
        "plt.ylabel('Variabilidade Acumulada')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzR3LxhRzUgI"
      },
      "source": [
        "### 02. Selecionar valores apropriados de variabilidade acumulada para reduzir a dimensionalidade do problema de classificação, resolvendo-o para o respectivo banco de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7KjREnzjx2"
      },
      "source": [
        "Para realizar a redução de dimensionalidade, calculamos o número de autovalores necessários para uma porcentagem de 95% em variabilidade acumulada, utilizando compreensão de listas e a função enumerate. Como resultado, obtivemos que 185 valores singulares eram necessários para tal objetivo.\n",
        "\n",
        "Com isso, calculamos a nova matriz V, com apenas 185 colunas, e calculamos a projeção da matriz X_train(24000x1024) e X_test(16000x1024) na nova V. Assim, obtivemos uma matriz Y_train de 24000 por 185 e uma Y_test de 16000 por 185"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkGe9WkPzWnJ"
      },
      "source": [
        "porcentagem = 0.95\n",
        "\n",
        "# Armazena o primeiro elemento da lista do número de valores singulares\n",
        "# para variabilidade acumulada > porcentagem\n",
        "r = [n for n, i in enumerate(variabilidade_acumulada) if i > porcentagem][0]\n",
        "\n",
        "print( 'Variabilidade acumulada de {:.2f}% com {} Valores Singulares\\n' \\\n",
        "      .format(100 * variabilidade_acumulada[r], r) )\n",
        "\n",
        "# Redução de dimensionalidade\n",
        "V_chapeu = Vt[:r,:].T \n",
        " \n",
        "Y_test = X_test @ V_chapeu\n",
        "\n",
        "Y_train = X_train @ V_chapeu\n",
        "\n",
        "print( 'Dimensões Antes:\\nX_train: ', X_train.shape, \\\n",
        "       '\\nX_test: ', X_test.shape, \\\n",
        "       '\\nV: ', Vt.shape)\n",
        "\n",
        "print( '\\nDimensões Depois:\\nY_train: ', Y_train.shape, \\\n",
        "       '\\nY_test: ', Y_test.shape, \\\n",
        "       '\\nV_chapeu: ', V_chapeu.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0MOD4TLBMlx"
      },
      "source": [
        "Para classificar as imagens, utilizamos o método kNN (k vizinhos mais próximos), considerando k=1, o qual consiste em achar o vetor mais próximo da imagem de teste que queremos classificar. Para isso, utilizamos a função norm do numpy.linalg com o argumento axis=1, que retorna um array com as diferenças entre o vetor teste e todos os vetores do X_train. \n",
        "\n",
        "Após isso, armazenamos a posição da menor distância com a função np.argmin, e verificamos o resultado comparando o y_train do vetor mais parecido com o y_test do vetor que estamos querendo classificar. Por fim, temos o número de classificações corretas e podemos verificar a acurácia. \n",
        "\n",
        "Utilizando uma variabilidade acumulada de 95% e a redução de dimensionalidade com 185 autovalores, foi obtida uma acurácia de 93.50%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKAPI4EcjCEt"
      },
      "source": [
        "# Classificação de Imagens\n",
        "\n",
        "corretos = 0\n",
        "error_ids = []\n",
        "for i, vetor_test in enumerate(Y_test):\n",
        "    # Distância entre cada vetor da matriz de treino e o vetor da matriz de teste\n",
        "    distances = np.linalg.norm(Y_train - vetor_test, axis=1)\n",
        "    # Vetor mais parecido\n",
        "    mais_parecido = np.argmin(distances)\n",
        "    # Verifica Resultado\n",
        "    if y_train[mais_parecido] == y_test[i]:\n",
        "        corretos += 1\n",
        "    else:\n",
        "        error_ids.append(i)\n",
        "        \n",
        "print('Acurácia: {:.2f}%'.format(100 * corretos/16000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpBi-4IyVsYy"
      },
      "source": [
        "Exemplos do resultado da classificação:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05zkwGHJVlgq"
      },
      "source": [
        "# Funções - Exemplo de Classificação\n",
        "\n",
        "# Plota o gráfico com a imagem real e a mais parecida\n",
        "def comparar_imagens(real, mais_parecido):\n",
        "    fig = plt.figure(figsize=(8,5))\n",
        "    fig.suptitle('Comparação de Resultados', fontsize=14)\n",
        "\n",
        "    fig.add_subplot(1,2,1)\n",
        "    plt.title('Imagem original')\n",
        "    plt.imshow(real, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    fig.add_subplot(1,2,2)\n",
        "    plt.title('Imagem mais parecida')\n",
        "    plt.imshow(mais_parecido, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "# Imprime se a imagem contém rachadura ou não\n",
        "# y = 1 -> Contém rachadura\n",
        "# y = 0 -> Não contém rachadura\n",
        "def imprimir_resultados(id, pos):\n",
        "\n",
        "    resultados = []\n",
        "    \n",
        "    if y_test[id] == 1:\n",
        "        resultados.append('Contém Rachadura')\n",
        "    else:\n",
        "        resultados.append('Não Contém Rachadura')\n",
        "        \n",
        "    if y_train[pos] == 1:\n",
        "        resultados.append('Contém Rachadura')\n",
        "    else:\n",
        "        resultados.append('Não Contém Rachadura')\n",
        "\n",
        "    print('Resultados:\\nImagem original: ', resultados[0], \\\n",
        "          '\\nImagem mais parecida: ', resultados[1])\n",
        "\n",
        "    \n",
        "# Classifica a imagem e mostra os resultados\n",
        "def classificar(id):\n",
        "    \n",
        "    vetor_test = Y_test[id]\n",
        "    \n",
        "    # Distância entre cada vetor da matriz de treino e o vetor da matriz de teste\n",
        "    distances = np.linalg.norm(Y_train - vetor_test, axis=1)\n",
        "    \n",
        "    # Vetor mais parecido\n",
        "    pos = np.argmin(distances)\n",
        "    \n",
        "    # Restaura a média\n",
        "    X_train_image = X_train + X_train_mean\n",
        "    X_test_image = X_test + X_test_mean\n",
        "\n",
        "    # Vetores do resultado\n",
        "    mais_parecido = X_train_image[pos]\n",
        "    real = X_test_show[id]\n",
        "    \n",
        "    # Torna os vetores 32x32\n",
        "    mais_parecido = mais_parecido.reshape((32, 32))\n",
        "    real = real.reshape((32,32))\n",
        "    \n",
        "    # Gŕafico com resultados\n",
        "    comparar_imagens(real,mais_parecido)\n",
        "    # Resultados\n",
        "    imprimir_resultados(id, pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hReeGwMJVm0I"
      },
      "source": [
        "# Exemplo - resultado correto\n",
        "classificar(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l4CROaYVoRC"
      },
      "source": [
        "print('Exemplo de erros: ', error_ids[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUbh2sM8Vp2h"
      },
      "source": [
        "# Exemplo - resultado errado\n",
        "classificar(108)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ichf1eK7zXAo"
      },
      "source": [
        "### 03. Gerar um gráco de número de valores singulares versus acurácia. Isso deve ser feito de maneira apropriada, não devendo o gráco ser gerado em tempo superior a 24 horas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHcgzr-21FbN"
      },
      "source": [
        "Para gerar o gráfico de valores singulares X acurácia, definimos duas funções, a componentes_principais, que recebe o número de autovalores para a diminuição de dimensionalidade e retorna as novas matrizes Y_train e Y_test reduzidas, e a find_match, que utiliza a biblioteca numba para um processamento mais rápido e realiza a classificação, recebendo como argumento as matrizes reduzidas, as matrizes ytrain e ytest, que armazenam as classes de cada imagem, o número de vetores teste e o número de vetores treino, e retorna a acurácia da classificação.\n",
        "\n",
        "De posse dessas funções, realizamos a classificação no range de 1 a 1024 valores singulares, pulando de 5 em 5, e guardamos a acurácia em uma lista, processo realizado em 23 horas, 32 minutos e 36 segundos. Após o processamento, foi possível criar o gráfico de valores singulares X acurácia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtosmekRzZsS"
      },
      "source": [
        "def componentes_principais(r):\n",
        "    V_chapeu = Vt[:r,:].T \n",
        "    Y_test = X_test @ V_chapeu\n",
        "    Y_train = X_train @ V_chapeu\n",
        "    \n",
        "    return Y_train, Y_test\n",
        "\n",
        "@njit(fastmath=True, cache=True)\n",
        "def find_match(Ytrain, Ytest, ytrain, ytest, ntrains, ntests):\n",
        "    count = 0\n",
        "    for i in range(ntests):\n",
        "        min_dist = 10000\n",
        "        for j in range(ntrains):\n",
        "            dist = np.linalg.norm(Ytest[i,:] - Ytrain[j,:])\n",
        "            if dist <= min_dist:\n",
        "                index = j\n",
        "                min_dist = dist\n",
        "        if ytrain[index] == ytest[i]:\n",
        "            count += 1\n",
        "    return count / ntests\n",
        "\n",
        "acuracia = []\n",
        "\n",
        "inicio = time.time()\n",
        "\n",
        "for i in range(1, 1024, 5):\n",
        "    Y_train, Y_test = componentes_principais(i)\n",
        "    count = find_match(Y_train, Y_test, y_train, y_test, Y_train.shape[0], Y_test.shape[0])\n",
        "    acuracia.append(count)\n",
        "\n",
        "tempo_total = time.time() - inicio\n",
        "\n",
        "print('Tempo de execução: ', time.strftime('%H:%M:%S', time.gmtime(tempo_total)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbLxkzx-jT62"
      },
      "source": [
        "# Gráfico\n",
        "y_plot = acuracia\n",
        "x_plot = list(range(1,1024,5))\n",
        "\n",
        "plt.plot(x_plot, y_plot)\n",
        "plt.xlabel('Número de valores singulares')\n",
        "plt.ylabel('Acurácia(%)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_AwbeSLzZ2C"
      },
      "source": [
        "## Questões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kKcbcmnzb1C"
      },
      "source": [
        "### 02. Vericar numericamente a proposição anterior, calculando, para o respectivo banco de dados, os autovalores de $X^TX$ e $XX^T$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpGm4UbS22nr"
      },
      "source": [
        "Com o objetivo de obter um melhor processamento, utilizamos apenas as primeiras 5000 colunas da matriz de treino, e calculamos e ordenamos em ordem decrescente as matriz de autovalores e autovetores das matrizes $X^TX$ e $XX^T$\n",
        "\n",
        "Para obter a matriz sem os autovalores nulos de $XX^T$, armazenamos apenas as 1024 primeiras colunas (as unícas com valores não nulos, após ordenamento decrescente). \n",
        "\n",
        "Afim de verificar a proposição da questão 1, subtraímos a matriz de autovalores de $X^TX$ com a matriz de autovalores não nulos de $XX^T$, e obtivemos uma diferença de 2.9103830456733704e-11, desprezível, resultante da impressão dos cálculos. Portanto, os autovalores não nulos são iguais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHi0CcV3zdJS"
      },
      "source": [
        "# Calcula os autovalores e vetores de xxt e xtx:\n",
        "X=X_train[:5000:]\n",
        "\n",
        "w1, q1 = np.linalg.eigh(X.T @ X)\n",
        "w2, q2 = np.linalg.eigh(X @ X.T)\n",
        "\n",
        "# Ordena os autovalores em ordem decrescente:\n",
        "indices1 = np.argsort(w1)[::-1]\n",
        "autovalores_decrescentes1 = w1[indices1]\n",
        "\n",
        "indices2 = np.argsort(w2)[::-1]\n",
        "autovalores_decrescentes2 = w2[indices2]\n",
        "\n",
        "# Exclui os valores nulos da matriz xxt\n",
        "autovalores_decrescentes2 = autovalores_decrescentes2[:1024:]\n",
        "\n",
        "#armazena os dados e imprime o resultado\n",
        "print(f'Autovalores XtX:\\n{autovalores_decrescentes1}\\n')\n",
        "print(f'Aautovalores XXt:\\n{autovalores_decrescentes2}\\n')\n",
        "print(f'Diferença: {np.max(np.abs( autovalores_decrescentes1 - autovalores_decrescentes2))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr9cM45Kzdq7"
      },
      "source": [
        "### 04. Vericar a proposição anterior, comparando, para o respectivo banco de dados, a matriz de autovetores de $X^TX$ e a matriz de vetores singulares direitos, isto é, $V$ em $X = USV^T$. Atenção para com os sentidos dos vetores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK6P3MN2ai3i"
      },
      "source": [
        "Para provarmos que $X^TX$ tem os mesmo autovetores de V provido pela SVD, calculamos os autovetores(Q) com a função np.linalg.eigh, e ordenamos os valores absolutos de ambas as matrizes V e Q. Após isso, calculamos a diferença com a função np.max, que retornou 1.2541135664678738e-10, um número muito pequeno, provido da imprecisão dos cálculos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmQKSwLNzflC"
      },
      "source": [
        "# Obtém os autovalores e autovetores\n",
        "w, Q = np.linalg.eigh(X_train.T @ X_train)\n",
        "\n",
        "# Utiliza o V provido da SVD\n",
        "# Ordena os valores absolutos de ambas as matrizes\n",
        "V = Vt.T\n",
        "V  = np.sort(np.abs(V), axis=1)\n",
        "Q = np.sort(np.abs(Q), axis=1)\n",
        "\n",
        "# Resultado - Diferença\n",
        "print(f'Diferença = {np.max(np.abs(V - Q))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUBL5TQ5zfvr"
      },
      "source": [
        "### 05. Comparar, para o respectivo banco de dados, a matriz $Q$ de autovetores de $XX^T$ e a matriz de vetores singulares esquerdos, isto é, $U$ em $X = USV^T$. Justique por que as duas matrizes são diferentes, mas as submatrizes $U[:, 0 : t]$ e $Q[:, 0 : t]$ são iguais (t é o índice do primeiro autovalor nulo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0MeHiEyCZJ9"
      },
      "source": [
        "Reduzindo a matriz de treino para 3000x1024, para fins de melhor processamento, foi realizada a svd da matriz completa, para obter U de 3000x3000, e a decomposição espectral de $XX^T$ para obter o Q de 3000x3000. Após isso, foi realizado o ordenamento dos valores absolutos de ambas as matrizes, e calculada a diferença entre elas, resultando em 0.8868200676499786 e a diferença entre as submatrizes utilizando t = 1024, 0.006740097275460629."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfNCNka2zhZt"
      },
      "source": [
        "X = X_train[:3000:]\n",
        "\n",
        "# SVD completa\n",
        "u, s, vt = np.linalg.svd(X)\n",
        "\n",
        "# Decomposição espectral\n",
        "w, q = np.linalg.eigh(X @ X.T)\n",
        "\n",
        "# Ordenamento\n",
        "u = np.sort(np.abs(u), axis = 1)\n",
        "q = np.sort(np.abs(q), axis = 1)\n",
        "\n",
        "print(f'Diferença matrizes: {np.max(u-q)}')\n",
        "\n",
        "u = u[:,0:1024]\n",
        "q = q[:,0:1024]\n",
        "\n",
        "print(f'Diferença submatrizes: {np.max(u-q)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}